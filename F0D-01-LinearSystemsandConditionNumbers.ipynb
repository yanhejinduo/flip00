{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLIP (00): Data Science \n",
    "**(Module 03: Linear Algebra)**\n",
    "\n",
    "---\n",
    "\n",
    "- Materials in this module include resources collected from various open-source online repositories.\n",
    "- You are free to use,but NOT allowed to change and distribute this package.\n",
    "\n",
    "Prepared by and for \n",
    "**Student Members** |\n",
    "2006-2023 [TULIP Lab](http://www.tulip.org.au), Australia\n",
    "\n",
    "---\n",
    "\n",
    "## Session 9 Linear Systems and Condition Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in solving linear systems of equations which we write in the form\n",
    "\n",
    "\\begin{equation}\n",
    "  A {\\bf x} = {\\bf b},\n",
    "\\end{equation}\n",
    "\n",
    "where $A$ is a known matrix, ${\\bf b}$ a known vector, and ${\\bf x}$ is the unknown solution vector that we are trying to find. By convention the system has size $n$ - that is, the matrix has size $n \\times n$ and the vectors are column vectors length $n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before attempting to solve the linear system, we recall that not all linear systems can be solved. If the matrix is singular, which is equivalent to $\\det(A) = 0$, then the matrix cannot be inverted, and there is no solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider the accuracy with which the coefficients of $A$ and ${\\bf b}$ can be known. Real numbers cannot be stored on a computer to infinite precision, so the coefficients cannot be accurate beyond, for example, 16 significant figures. In most interesting cases even this is too optimistic: the coefficients of $A$ and ${\\bf b}$ will usually be the result of some other numerical or experimental procedure with its own inherent inaccuracies. \n",
    "\n",
    "This implies one crucial point. There may be linear systems defined by $\\left( A, {\\bf b} \\right)$ that we do not want to solve, as it is _impossible_ to be sure that the solution is sufficiently accurate. That is, a \"small\" change in the coefficients of e.g. $A$, can lead to a \"large\" change in the solution. We would usually interpret \"small\" to mean within the accuracy with which we know the coefficients. What \"large\" means depends on the accuracy we require on the solution, and is problem dependent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the linear system does have a reliable solution - that is, small changes in the coefficients lead to small changes in the solution - we call it **well conditioned**. If it does not, we call it **badly conditioned**. If the linear system is badly conditioned then it cannot be reliably solved: find a different problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cannot, practically, solve not just one linear system, but many neighbouring problems, just to check if the system is reasonable. Instead we want a simple criterion that we can cheaply check to see whether it is worth solving the linear system. We can condense this down to a single number, called the **condition number**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why not use the determinant as the condition number? After all, if the determinant is zero the matrix cannot be inverted. However, consider the following matrix:\n",
    "\n",
    "\\begin{equation}\n",
    "  A = 10^{-1} \\begin{pmatrix} 1 & 0 & \\dots & 0 \\\\ 0 & 1 & \\ddots & \\vdots \\\\ \\vdots & \\ddots & \\ddots & 0 \\\\ 0 & \\dots & 0 & 1 \\end{pmatrix}.\n",
    "\\end{equation}\n",
    "\n",
    "This diagonal matrix is trivially inverted and perfectly well behaved, no matter how large. However, $\\det(A) = 10^{-n}$, which is arbitrarily small for sufficiently large $n$. So the determinant cannot be a good condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead, let us consider what happens if we perturb the coefficients. For simplicity, just perturb the coefficients of the matrix. We do this by considering the matrix $A + \\epsilon F$, where $F$ is some arbitrary matrix and $\\epsilon$, assumed small, is the \"size\" of the perturbation. Now the solution will depend on the perturbation: write the solution as ${\\bf x}(\\epsilon)$. So the problem we are looking to solve is\n",
    "\n",
    "\\begin{equation}\n",
    "  \\left( A + \\epsilon F \\right) {\\bf x}(\\epsilon) = {\\bf b}.\n",
    "\\end{equation}\n",
    "\n",
    "We are interested in how much the solution changes as we vary $\\epsilon$, or in particular the difference between the \"true\" solution (when $\\epsilon = 0$) and the perturbed solution. Using Taylor expansion (about $\\epsilon = 0$) we can write\n",
    "\n",
    "\\begin{equation}\n",
    "  {\\bf x}(\\epsilon) - {\\bf x}(0) = \\epsilon \\left. \\dot{\\bf x} \\right|_{\\epsilon = 0} + {\\cal O} \\left( \\epsilon^2 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "Going back to the definition of the problem we see\n",
    "\n",
    "\\begin{align}\n",
    "  && \\left( A + \\epsilon F \\right) \\dot{\\bf x} + F {\\bf x} & = 0 \\\\\n",
    "  \\implies && \\left. \\dot{\\bf x} \\right|_{\\epsilon = 0} &= - A^{-1} F {\\bf x}(0) + {\\cal O} \\left( \\epsilon \\right).\n",
    "\\end{align}\n",
    "\n",
    "Therefore the *absolute* error induced by the perturbation is\n",
    "\n",
    "\\begin{equation}\n",
    "  {\\bf x}(\\epsilon) - {\\bf x}(0) = -\\epsilon A^{-1} F {\\bf x}(0) + {\\cal O} \\left( \\epsilon^2 \\right).\n",
    "\\end{equation}\n",
    "\n",
    "More interesting is the size of the relative error, which is\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{\\| {\\bf x}(\\epsilon) - {\\bf x}(0) \\|}{\\| {\\bf x} \\|} \\sim \\epsilon \\| A^{-1} F \\| \\sim \\| A^{-1} \\| \\| A \\| \\left( \\epsilon \\frac{\\| F \\|}{\\| A \\|} \\right).\n",
    "\\end{equation}\n",
    "\n",
    "The final form groups the terms into those we know, or can control (which is just the matrix $A$), and those we do not (the terms corresponding to the perturbation, which are $\\epsilon$ and $F$). The unknown terms are also scaled to be dimensionless by \"dividing\" the unknown $F$ by the known $A$.\n",
    "\n",
    "This final form gives us our condition number: it is the piece that we can calculate\n",
    "\n",
    "\\begin{equation}\n",
    "  K(A) = \\| A^{-1} \\| \\| A \\|.\n",
    "\\end{equation}\n",
    "\n",
    "However, we have not yet determined how to compute the \"magnitude\" or \"norm\" of a matrix, $\\| A \\|$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector and matrix norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A norm is a mathematical distance function. Standard norms for e.g. real vectors use the sizes of the components or the \"length\" of the vector. The most useful for our purposes are the $1, 2$- and $\\infty$-norms:\n",
    "\n",
    "\\begin{align}\n",
    "  \\| x \\|_{1} & = \\sum_{j = 1}^n | x_j |, \\\\\n",
    "  \\| x \\|_{2} & = \\sqrt{\\sum_{j = 1}^n ( x_j )^2}, \\\\\n",
    "  \\| x \\|_{\\infty} & = \\max_{j} | x_j |.\n",
    "\\end{align}\n",
    "\n",
    "For example, the $2$-norm is the \"standard\" Pythagorean distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that different norms give different answers when applied to the *same* vector. For example, if ${\\bf x} = (-1, 2, 1)$ then\n",
    "\n",
    "\\begin{align}\n",
    "  \\| x \\|_{1} & = |-1| + |2| + |1| && = 4, \\\\\n",
    "  \\| x \\|_{2} & = \\sqrt{(-1)^2 + (2)^2 + (1)^2} && = \\sqrt{6}, \\\\\n",
    "  \\| x \\|_{\\infty} & = \\max_{j} \\left(|-1|, |2|, |1| \\right) && = 4.\n",
    "\\end{align}\n",
    "\n",
    "Hence, when comparing norms of different vectors, it is necessary to always use the same norm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an illustration of the different norms, let us restrict to 2 dimensional vectors (which are easy to visualize) and plot all vectors with norm 1 for each of these three norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg as la\n",
    "\n",
    "x1 = np.linspace(-1.5, 1.5, 100)\n",
    "x2 = x1\n",
    "[X, Y] = np.meshgrid(x1, x2)\n",
    "\n",
    "N1   = np.zeros_like(X)\n",
    "N2   = np.zeros_like(X)\n",
    "Ninf = np.zeros_like(X)\n",
    "\n",
    "for i in range(len(x1)):\n",
    "    for j in range(len(x1)):\n",
    "        v = np.array([x1[i], x2[j]])\n",
    "        N1[i, j]   = la.norm(v, 1)\n",
    "        N2[i, j]   = la.norm(v, 2)\n",
    "        Ninf[i, j] = la.norm(v, np.inf)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.contour(X, Y, N1, levels = [1])\n",
    "plt.title(r\"$\\| {\\bf x} \\|_{1} = 1$\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.contour(X, Y, N2, levels = [1])\n",
    "plt.title(r\"$\\| {\\bf x} \\|_{2} = 1$\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.contour(X, Y, Ninf, levels = [1])\n",
    "plt.title(r\"$\\| {\\bf x} \\|_{\\infty} = 1$\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.tight_layout(pad = 1.0)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Norms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We really want matrix norm, $\\| A \\|$. We can use any vector ${\\bf y}$ combined with any vector norm to *induce* a matrix norm by defining\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\|_{\\bf y} = \\| A {\\bf y} \\|\n",
    "\\end{equation}\n",
    "\n",
    "where the norm on the right hand side is a vector norm. Of course, this definition depends on the choice of vector ${\\bf y}$. So we could maximize over *all* vectors ${\\bf y}$: however, the size of the norm depends on the size of the vector. Therefore we want to take the magnitude of the vector ${\\bf y}$ out: we define the **induced matrix norm** as\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\| = \\max_{{\\bf y}: \\| {\\bf y} \\| = 1} \\| A {\\bf y} \\|.\n",
    "\\end{equation}\n",
    "\n",
    "Throughout this definition, all norms *must* be the same. For example, the matrix $2$-norm is given by\n",
    "\n",
    "\\begin{equation}\n",
    "  \\| A \\|_2 = \\max_{{\\bf y}: \\| {\\bf y} \\|_2 = 1} \\| A {\\bf y} \\|_2.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually hand calculating the matrix norm using this definition is obviously difficult. However, it can be shown that two particular norms can be greatly simplified. That is\n",
    "\n",
    "1. the matrix $1$-norm is given by the maximum of the $1$-norm of the *column* vectors of $A$;\n",
    "2. the matrix $\\infty$-norm is given by the maximum of the $1$-norm of the *row* vectors of $A$.\n",
    "\n",
    "Note that, in contrast to everything else in this section, the $1$-norm is used for the vectors in both cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally we now have all the tools needed to compute our condition number. Again the condition number depends on the norm used in its calculation, but we would expect that all results have a similar magnitude.\n",
    "\n",
    "Loosely we interpret the condition number to measure *the amount that inverting the matrix will increase any intrinsic error in the coefficients*.\n",
    "\n",
    "More precisely, we can look at the relative error that we can measure: the *weighted residual*\n",
    "\n",
    "\\begin{equation}\n",
    "  {\\cal E} = \\frac{\\| A {\\bf x}_{\\text{num}} - {\\bf b}\\|}{\\| {\\bf b} \\|}.\n",
    "\\end{equation}\n",
    "\n",
    "Here ${\\bf x}_{\\text{num}}$ is the numerical solution. In principle, the weighted residual could be minimized, but we can never guarantee that it is precisely zero.\n",
    "\n",
    "Now, it can be shown that\n",
    "\n",
    "\\begin{equation}\n",
    "  \\frac{1}{K(A)} {\\cal E} \\le \\frac{\\| {\\bf x}_{\\text{num}} - {\\bf x}_{\\text{exact}} \\|}{\\| {\\bf x}_{\\text{exact}} \\|} \\le K(A) {\\cal E}.\n",
    "\\end{equation}\n",
    "\n",
    "The lower bound is not important here: the point is the upper bound. Assume we have minimized the weighted residual to, for example, $10^{-16}$. Then, if the condition number is $\\sim 10^{15}$ then the best bound on the relative error is $\\sim 0.1$: in other words, we can only guarantee the correctness of the *first* significant digit of our solution!"
   ]
  }
 ],
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
